import cv2
import mediapipe as mp
import pyautogui
import numpy as np

def run_gaze_estimation(q, show_face_mesh=False, stop_event=None):
    screen_w, screen_h = pyautogui.size()   # í˜„ì¬ ëª¨ë‹ˆí„° í•´ìƒë„
    cap = cv2.VideoCapture(0)               # ë…¸íŠ¸ë¶ ì¹´ë©”ë¼ ìº¡ì³
    print(f"Gaze Estimation, Camera resolution: {cap.get(cv2.CAP_PROP_FRAME_WIDTH)} x {cap.get(cv2.CAP_PROP_FRAME_HEIGHT)}")

    mp_face_mesh = mp.solutions.face_mesh                       # ì–¼êµ´ ì¸ì‹ í´ë˜ìŠ¤
    face_mesh = mp_face_mesh.FaceMesh(refine_landmarks=True)    # ì–¼êµ´ ë©”ì‰¬ ì¤‘ refine_landmarks ë°˜ì˜(í™ì±„)

    if show_face_mesh:        # ì–¼êµ´ ë©”ì‰¬ ë¶ˆëŸ¬ì˜¬ ë•Œ
        mp_drawing = mp.solutions.drawing_utils                 # ì–¼êµ´ì— ê·¸ë¦¬ëŠ” ê¸°ëŠ¥
        mp_drawing_styles = mp.solutions.drawing_styles         # ê·¸ë¦¬ëŠ” ìŠ¤íƒ€ì¼ ì…‹ ê°€ì ¸ì˜´
        cv2.namedWindow("Face Mesh View", cv2.WINDOW_NORMAL)    # "Face Mesh View" ì°½ ì—´ê¸°

    cv2.namedWindow("Gaze Estimation", cv2.WINDOW_NORMAL)       # "Gaze Estimation" ì°½ ì—´ê¸°

    calibration_step = 0            # í˜„ì¬ ë³´ì • ì¤‘ì¸ ì½”ë„ˆ
    calibration_complete = False    # ë³´ì • ì™„ë£Œ ì—¬ë¶€
    calibration_points = []         # ìˆ˜ì§‘ëœ ëˆˆ ìœ„ì¹˜ ì €ì¥

    margin = 50         # ëª¨ì„œë¦¬ ì¢Œí‘œ ê¸°ì¤€
    corner_points = [
        (margin, margin),  # top-left
        (screen_w - margin, margin),  # top-right
        (margin, screen_h - margin),  # bottom-left
        (screen_w - margin, screen_h - margin)  # bottom-right
    ]
    instructions = [    # ê° ëª¨ì„œë¦¬ ì•ˆë‚´
        "Look at top-left corner and press 'w'",
        "Look at top-right corner and press 'w'",
        "Look at bottom-left corner and press 'w'",
        "Look at bottom-right corner and press 'w'"
    ]

    # ë³´ê°„ìš© ì´ˆê¸°í™”
    prev_x, prev_y = None, None
    alpha = 0.5 # ë¶€ë“œëŸ¬ì›€ ì •ë„ (0.1 ~ 0.3 ì¶”ì²œ)

    while True:
        if stop_event and stop_event.is_set():  # Stop eventê°€ ì‹¤í–‰ë˜ë©´
            print("stop_event ë¥¼ í†µí•œ ë£¨í”„ ì¢…ë£Œ")
            break                               # ì¢…ë£Œ

        ret, frame = cap.read()                 # ret = í”„ë ˆì„ ì •ìƒì ìœ¼ë¡œ ì½ì—ˆëŠ”ì§€ ì—¬ë¶€, frame = ì˜ìƒë°ì´í„°
        if not ret:                             # ì •ìƒì ìœ¼ë¡œ ëª» ì½ìœ¼ë©´
            break                               # ì¢…ë£Œ

        frame = cv2.flip(frame, 1)              # frame ì˜ ì´ë¯¸ì§€ë¥¼ ì¢Œìš° ë°˜ì „
        face_mesh_frame = frame.copy() if show_face_mesh else None  # ë©”ì‰¬ë¥¼ ë³¸ë‹¤ë©´, frameì„ ë³µì‚¬í•˜ì—¬ ì €ì¥

        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # BGRì„ RGBë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥
        results = face_mesh.process(rgb_frame)

        screen = np.ones((screen_h, screen_w, 3), dtype=np.uint8) * 255

        if results.multi_face_landmarks:
            for face_landmarks in results.multi_face_landmarks:
                if show_face_mesh and face_mesh_frame is not None:
                    mp_drawing.draw_landmarks(
                        image=face_mesh_frame,
                        landmark_list=face_landmarks,
                        connections=mp_face_mesh.FACEMESH_TESSELATION,
                        landmark_drawing_spec=None,
                        connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_tesselation_style()
                    )
                    mp_drawing.draw_landmarks(
                        image=face_mesh_frame,
                        landmark_list=face_landmarks,
                        connections=mp_face_mesh.FACEMESH_IRISES,
                        landmark_drawing_spec=None,
                        connection_drawing_spec=mp_drawing.DrawingSpec(color=(0,255,0), thickness=1)
                    )
                left_iris = face_landmarks.landmark[468]
                right_iris = face_landmarks.landmark[473]
                nose_tip = face_landmarks.landmark[1]

                eye_x = (left_iris.x + right_iris.x) / 2
                eye_y = (left_iris.y + right_iris.y) / 2

                dx = eye_x - nose_tip.x
                dy = eye_y - nose_tip.y

                gaze_x = int((0.5 + dx * 5) * screen_w)
                gaze_y = int((0.5 + dy * 5) * screen_h)

                gaze_x = np.clip(gaze_x, 0, screen_w - 1)
                gaze_y = np.clip(gaze_y, 0, screen_h - 1)

                # cv2.circle(screen, (gaze_x, gaze_y), 20, (255, 0, 0), -1)
                if not stop_event.is_set():
                    q.put((gaze_x, gaze_y))# âœ… IPCë¡œ ì „ì†¡

                if not calibration_complete:
                    if calibration_step < 4:
                        # ë³´ì •ìš© ì‹­ì í‘œì‹œ
                        cv2.circle(screen, corner_points[calibration_step], 30, (0, 0, 255), -1)
                        font = cv2.FONT_HERSHEY_SIMPLEX
                        text = instructions[calibration_step]
                        (tw, th), _ = cv2.getTextSize(text, font, 0.8, 2)
                        tx = (screen_w - tw) // 2
                        ty = screen_h // 2
                        cv2.putText(screen, text, (tx, ty), font, 0.8, (0, 0, 0), 2)
                else:
                    # ëˆˆ ìœ„ì¹˜ë¥¼ ë³´ì •ëœ í™”ë©´ ì¢Œí‘œë¡œ ë§¤í•‘
                    xs, ys = zip(*calibration_points)
                    min_x, max_x = min(xs), max(xs)
                    min_y, max_y = min(ys), max(ys)

                    range_x = max_x - min_x
                    range_y = max_y - min_y

                    dx = eye_x
                    dy = eye_y

                    if range_x == 0 or range_y == 0:
                        mapped_x, mapped_y = screen_w // 2, screen_h // 2
                    else:
                        mapped_x = int((dx - min_x) / range_x * screen_w)
                        mapped_y = int((dy - min_y) / range_y * screen_h)

                    mapped_x = np.clip(mapped_x, 0, screen_w - 1)
                    mapped_y = np.clip(mapped_y, 0, screen_h - 1)

                    if prev_x is not None and prev_y is not None:   #ì²˜ìŒ ì‹œì‘í•  ë•Œ ì´ì „ ì¢Œí‘œê°€ ì—†ê¸° ë•Œë¬¸ì—, ìˆì„ ë•Œì—ë§Œ ì‹¤í–‰
                        mapped_x = int(alpha * mapped_x + (1 - alpha) * prev_x) #ì¢Œí‘œ = ì¡°ì •ê°’ * ìœ„ì¹˜ + (1-ì¡°ì •ê°’) * ì´ì „ ê°’
                        mapped_y = int(alpha * mapped_y + (1 - alpha) * prev_y) 

                    prev_x, prev_y = mapped_x, mapped_y     # ì´ì „ ê°’ = í˜„ì¬ ê°’

                    cv2.circle(screen, (mapped_x, mapped_y), 20, (0, 0, 255), -1)
                    if not stop_event.is_set():
                        q.put((mapped_x, mapped_y))



        key = cv2.waitKey(1) & 0xFF
        if key == ord('w') and not calibration_complete and calibration_step < 4:
            calibration_points.append((eye_x, eye_y))
            calibration_step += 1
            if calibration_step == 4:
                calibration_complete = True

        if key == ord('q'):
            if stop_event is not None:
                print("ğŸ›‘ Gaze Estimation ì¢…ë£Œ ì‹ í˜¸ ì „ì†¡")
                stop_event.set()
            break

        cv2.imshow("Gaze Estimation", screen)
        if show_face_mesh and face_mesh_frame is not None:
            cv2.imshow("Face Mesh View", face_mesh_frame)

    cap.release()
    cv2.destroyAllWindows()